{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blanco-herrero/Interviews/blob/main/classifier_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUE8hF6QquzJ",
        "outputId": "69e10a88-7c59-4939-9377-5b5c170bb94a"
      },
      "source": [
        "# Text classification for FakeDetector using RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VqXtK2q6JhIg"
      },
      "outputs": [],
      "source": [
        "#https://www.tensorflow.org/tutorials/load_data/text\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JUPysP5NJhIl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "Imi3IGiQl6z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWymCEEgJhIm"
      },
      "outputs": [],
      "source": [
        "#PARAMETERS\n",
        "\n",
        "FILE_NAMES = ['true_tweets.txt', 'false_tweets.txt']\n",
        "#!wc negative.txt\n",
        "!wc true_tweets.txt\n",
        "!wc false_tweets.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YfRzJFvQJhIn"
      },
      "outputs": [],
      "source": [
        "#PARAMETERS\n",
        "\n",
        "BUFFER_SIZE = 50000  #Randomness\n",
        "BATCH_SIZE = 16  #check if neccesary\n",
        "TAKE_SIZE = 500 #Test data size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uwi6YxaNJhIm"
      },
      "outputs": [],
      "source": [
        "def labeler(example, index):\n",
        "  return example, tf.cast(index, tf.int64)\n",
        "\n",
        "labeled_data_sets = []\n",
        "\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  lines_dataset = tf.data.TextLineDataset(os.path.join(file_name))\n",
        "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
        "  labeled_data_sets.append(labeled_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IssHYgWvJhIn"
      },
      "outputs": [],
      "source": [
        "#1 = Positive  0= Negative\n",
        "\n",
        "all_labeled_data = labeled_data_sets[0]\n",
        "for labeled_dataset in labeled_data_sets[1:]:\n",
        "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
        "\n",
        "all_labeled_data = all_labeled_data.shuffle(\n",
        "    BUFFER_SIZE, reshuffle_each_iteration=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4wTSVlcJhIn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for ex in all_labeled_data.take(10):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "K-97RTkrJhIo"
      },
      "outputs": [],
      "source": [
        "all_labeled_data = all_labeled_data.prefetch(2)\n",
        "\n",
        "train_data = all_labeled_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
        "train_data = train_data.prefetch(2)\n",
        "\n",
        "test_data = all_labeled_data.take(TAKE_SIZE)\n",
        "test_data = test_data.prefetch(2)\n",
        "\n",
        "train_size = len(list(train_data))\n",
        "test_size = len(list(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFhR9IOxJhIo"
      },
      "outputs": [],
      "source": [
        "print(\"all_labeled_data:\")\n",
        "print(type(all_labeled_data))\n",
        "print(all_labeled_data)\n",
        "print(len(list(all_labeled_data)))\n",
        "print(\"train_data:\")\n",
        "print(type(train_data))\n",
        "print(train_data)\n",
        "print(len(list(train_data)))\n",
        "print(\"test_data:\")\n",
        "print(type(test_data))\n",
        "print(test_data)\n",
        "print(len(list(test_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLvSIBP8JhIo"
      },
      "outputs": [],
      "source": [
        "#Examples of train data\n",
        "for X_batch, y_batch in train_data.batch(25).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Tweet / Headline:\", review.decode(\"utf-8\")[:10000])\n",
        "        print(\"Label:\", label, \"= FALSE_NEWS\" if label == 1 else \"= TRUE_NEWS\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzfQN1z5JhIp"
      },
      "outputs": [],
      "source": [
        "print(type(X_batch))\n",
        "print(X_batch)\n",
        "print(type(y_batch))\n",
        "print(y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "k7j8YxLeJhIp"
      },
      "outputs": [],
      "source": [
        "#PARAMETERS (AT LEAST THE NUMBER OF CHARACTERS)\n",
        "\n",
        "#Function to preprocess the train data\n",
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)  #Use the first 300 characters\n",
        "    X_batch = tf.strings.lower(X_batch)  #To lower case\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \") #Remove tags\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"http\\S+\", b\" \") #Remove html strings\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"[^\\P{P}]+\", b\" \") #Remove punctuation, except\n",
        "    X_batch = tf.strings.split(X_batch) #Split by spaces\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z_67OqQJhIp"
      },
      "outputs": [],
      "source": [
        "#Example of preprocessing of train data\n",
        "preprocess(X_batch, y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vnuyqI3eJhIp"
      },
      "outputs": [],
      "source": [
        "#Construct Vocabulary\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in train_data.batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2tBnBDqJhIr"
      },
      "outputs": [],
      "source": [
        "vocabulary.most_common()[:20]\n",
        "#CHECK if we should skip STOP WORDS or apply POS to use only N, V & A; OR Unify words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tbMQBeSJhIr"
      },
      "outputs": [],
      "source": [
        "len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "E0JX8h-oJhIr"
      },
      "outputs": [],
      "source": [
        "#PARAMETER\n",
        "\n",
        "#Truncate the vocabulary, keeping only the 10,000 most common words\n",
        "vocab_size = 2000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_wSBerUJhIs"
      },
      "outputs": [],
      "source": [
        "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "for word in b\"el psoe tacha de nausebunda la\".lower().split():\n",
        "    print(word_to_id.get(word) or vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_1cOI87-JhIs"
      },
      "outputs": [],
      "source": [
        "#PARAMETER\n",
        "\n",
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 100\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhdYkXx6JhIs"
      },
      "outputs": [],
      "source": [
        "table.lookup(tf.constant([u\"This is a test... I love the movie but it's too long\".lower().split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "y9nQVuf_JhIs"
      },
      "outputs": [],
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "12WY4Q88JhIs"
      },
      "outputs": [],
      "source": [
        "train_set = train_data.repeat().batch(32).map(preprocess)\n",
        "train_set = train_set.map(encode_words).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Wmd_WR5JJhIt"
      },
      "outputs": [],
      "source": [
        "test_set = test_data.repeat().batch(32).map(preprocess)\n",
        "test_set = test_set.map(encode_words).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfylVJEJJhIt"
      },
      "outputs": [],
      "source": [
        "for X_batch, y_batch in train_set.take(1):\n",
        "    print(X_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIJAwRhAJhIt"
      },
      "outputs": [],
      "source": [
        "#PARAMETERS\n",
        "\n",
        "embed_size = 6 #128 in the original example\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "                           mask_zero=False, # Check: this means that previous padding on test sets are expected\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", tf.keras.metrics.AUC(),\n",
        "                                                                     tf.keras.metrics.Precision(),\n",
        "                                                                    tf.keras.metrics.Recall()   ])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPjT_yuZJhIt"
      },
      "outputs": [],
      "source": [
        "#PARAMETER\n",
        "#Train the model\n",
        "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5, validation_data=test_set,\n",
        "                    validation_steps=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUvXaeUhJhIt"
      },
      "outputs": [],
      "source": [
        "#The F1 Score is the 2*((precision*recall)/(precision+recall)).\n",
        "#history.history\n",
        "loss = history.history[\"val_loss\"][-1]\n",
        "print(\"Loss =\", loss)\n",
        "accuracy = history.history[\"val_accuracy\"][-1]\n",
        "print(\"Accuracy =\", accuracy)\n",
        "precision = history.history[\"val_precision\"][-1]\n",
        "print(\"Precision = \", precision)\n",
        "recall = history.history[\"val_recall\"][-1]\n",
        "print(\"Recall =\", recall)\n",
        "f1 = 2*((precision*recall)/(precision+recall))\n",
        "print(\"F1 Score = \", f1)\n",
        "auc = history.history[\"val_auc\"][-1]\n",
        "print(\"ROC-AUC Score =\", auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFQ8JYOIJhIu"
      },
      "outputs": [],
      "source": [
        "print(plot_graphs(history, 'loss'))\n",
        "print(plot_graphs(history, 'accuracy'))\n",
        "print(plot_graphs(history, 'precision'))\n",
        "print(plot_graphs(history, 'recall'))\n",
        "print(plot_graphs(history, 'auc'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-CUEok7JhIu"
      },
      "outputs": [],
      "source": [
        "#CHECK HOW TO SAVE MODEL TO REUSE....\n",
        "#model.save(\"rnn_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hOOSZ2TJhIu"
      },
      "outputs": [],
      "source": [
        "def pad_to_size(vec, size):\n",
        "  zeros = [0] * (size - len(vec))\n",
        "  vec.extend(zeros)\n",
        "  return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tFWQG_KJhIu"
      },
      "outputs": [],
      "source": [
        "def encode_frase(sample_pred_text):\n",
        "    sample_pred_text = table.lookup(tf.constant([sample_pred_text.lower().split()]))\n",
        "    sample_pred_text = tf.make_tensor_proto(sample_pred_text)\n",
        "    sample_pred_text = tf.make_ndarray(sample_pred_text)\n",
        "    sample_pred_text = sample_pred_text.tolist()\n",
        "    sample_pred_text = sample_pred_text[0]\n",
        "    return sample_pred_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMiFC0YBJhIv"
      },
      "outputs": [],
      "source": [
        "def sample_predict(sample_pred_text, pad):\n",
        "  encoded_sample_pred_text = encode_frase(sample_pred_text)\n",
        "  if pad:\n",
        "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
        "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
        "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0), verbose=0)\n",
        "\n",
        "  return (predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjpeHcQOJhIv"
      },
      "outputs": [],
      "source": [
        "#PARAMETERS\n",
        "\n",
        "# predict on a sample text with or without padding: True or False\n",
        "\n",
        "sample_pred_text = \"Vladimir Putin destruye a la ideología de género en 5 minutos\"\n",
        "sample_pred_text2 = \"Pablo Casado nuevo líder del PP\"\n",
        "predictions = sample_predict(sample_pred_text, pad=True)\n",
        "predictions2 = sample_predict(sample_pred_text2, pad=True)\n",
        "print(predictions)\n",
        "print(predictions2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoBfqE9XPyXB"
      },
      "outputs": [],
      "source": [
        "#PARAMETERS\n",
        "\n",
        "#PRINT PREDICTIONS TO A FILE\n",
        "import sys\n",
        "original_stdout = sys.stdout\n",
        "\n",
        "with open('predictions.txt', 'w') as f:\n",
        "    sys.stdout = f\n",
        "    #Read cruella.txt from the same directory of the Notebook\n",
        "    f=open(\"my_file.txt\", \"r\", encoding=\"utf8\", errors='ignore')\n",
        "    f1=f.readlines()\n",
        "    for x in f1:\n",
        "        score = sample_predict(x, pad=True)\n",
        "        print(x.rstrip('\\n \\n'), \"\\t\", *score.flatten(), \"\\t\", \"Positive\" if score>0.5 else \"Negative\")\n",
        "    sys.stdout = original_stdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAW5sF5IQkkM"
      },
      "outputs": [],
      "source": [
        "!wc predictions.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fgcgk3nxsnYe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}